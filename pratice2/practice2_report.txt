Noms des membres de l'équipe :
- TACHER Adrien
- LECOMTE Solène
- GOUJON William

Nom de l'équipe : AdrienSoleneWilliam

----------------------------------------------------
## Exercice 1 : Indexation de la collection (practice2_exo1.py)

Cet exercice consistait à indexer plusieurs collections de tailles croissantes afin de mesurer le temps d’indexation et d’extraire quelques statistiques descriptives.

Le programme **practice2_exo1.py** réalise les étapes suivantes :
1. **read_documents()** : Extraction des blocs `<doc>` et `<docno>` à partir des fichiers .gz.
2. **tokeniser()** : Conversion en minuscules, suppression des ponctuations et chiffres (`[a-z]+`).
3. **build_index_and_stats()** : Construction de l’index inversé et calcul des statistiques :
   - Longueur moyenne des documents (avg_doc_length)
   - Longueur moyenne des termes (avg_term_length)
   - Taille du vocabulaire (vocab_size)
4. **Affichage** : Pour chaque fichier de collection, le programme affiche le temps d’indexation et les statistiques correspondantes.
5. **Graphique** : Génération du graphique “Indexing time vs collection size” (taille en Ko sur l’axe X, temps en secondes sur l’axe Y).

**Résultats observés :**
- Le temps d’indexation augmente avec la taille de la collection.
- Les statistiques restent stables sur les petits fichiers mais évoluent légèrement pour les grandes tailles.

----------------------------------------------------
## Exercice 2 : Statistiques de collection (practice2_exo2.py)

Le programme **practice2_exo2.py** reprend les étapes de lecture et de tokenisation de l’exercice 1, mais ne construit pas d’index complet. Il calcule uniquement des statistiques globales sur chaque fichier.

Étapes principales :
1. **read_documents()** : Extraction des documents.
2. **tokeniser()** : Nettoyage et normalisation des termes.
3. **build_stats()** : Calcul de trois indicateurs :
   - Longueur moyenne des documents (#terms)
   - Longueur moyenne des termes (#chars)
   - Taille du vocabulaire (#termes distincts)
4. **Graphiques générés :**
   - “Average document length vs size of collection”
   - “Average term length vs size of collection”
   - “Vocabulary size vs size of collection”

**Résultats observés :**
- La longueur moyenne des documents augmente légèrement avec la taille de la collection.
- La longueur moyenne des termes reste stable (autour de 5-6 caractères).
- La taille du vocabulaire croît presque linéairement.

----------------------------------------------------
## Exercice 3 : Suppression des stop-words (practice2_exo3.py)

Cet exercice visait à améliorer le prétraitement en retirant les **stop-words** (mots très fréquents) afin d’obtenir un index plus représentatif du contenu.

Le programme **practice2_exo3_stopwords.py** réalise les étapes suivantes :
1. **Chargement des stop-words** à partir du fichier `stop-words-english4.txt`.
2. **Tokenisation** : minuscules, suppression des caractères spéciaux et chiffres (`[a-z]+`).
3. **Filtrage** : exclusion des termes présents dans la liste des stop-words.
4. **Calcul des statistiques** identiques à celles de l’exercice 2.
5. **Affichage et graphiques** : comparaison de l’évolution des mesures après suppression des stop-words.

**Résultats observés :**
- Longueur moyenne des documents **diminue** (moins de mots non pertinents).
- Longueur moyenne des termes **augmente légèrement** (les mots restants sont plus “informatifs”).
- Taille du vocabulaire **diminue fortement** (suppression des mots fréquents comme “the”, “and”, “of”...).

----------------------------------------------------
## Exercice 4 : Stop-words + Porter’s Stemmer (practice2_exo4.py)

Cet exercice avait pour objectif d’aller plus loin dans le prétraitement des textes en appliquant la racinisation (stemming) avec l’algorithme de Porter après la suppression des stop-words.

Le programme practice2_exo4_stemmer.py réalise les étapes suivantes :
1. **Chargement des stop-words** à partir du fichier stop-words-english4.txt.
2. **Tokenisation** : conversion en minuscules, suppression des caractères spéciaux et des chiffres ([a-z]+).
3. **Filtrage + Stemming** : exclusion des mots présents dans la liste des stop-words et application du Porter Stemmer sur les termes restants.
4. **Calcul des statistiques** identiques à celles de l’exercice 2 et 3 :
5. **Affichage et graphiques** : comparaison de l’évolution des mesures avec et sans stemming.

**Résultats observés :**
- Longueur moyenne des documents diminue légèrement (les formes différentes d’un même mot sont fusionnées).
- Longueur moyenne des termes reste stable ou augmente légèrement (les mots restants sont plus “informative”).
- Taille du vocabulaire diminue fortement (le stemming regroupe plusieurs formes d’un même mot en une racine unique).